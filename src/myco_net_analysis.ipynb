from google.colab import files
uploaded = files.upload()

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score


# Load the dataset using the correct filename
df = pd.read_csv('plant_health_data (1).csv')

# Display the first 5 rows to confirm it loaded correctly
print("First 5 rows of the dataset:")
print(df.head())

# Check the shape of the dataset
print("\nDataset shape:", df.shape)

# Check basic information about the dataset
print("\nDataset info:")
print(df.info())

# Check for missing values
print("\nMissing values:")
print(df.isnull().sum())

# Check the distribution of the target variable
print("\nPlant Health Status distribution:")
status_counts = df['Plant_Health_Status'].value_counts()
print(status_counts)

# Visualize the distribution
plt.figure(figsize=(8, 6))
sns.barplot(x=status_counts.index, y=status_counts.values)
plt.title('Distribution of Plant Health Status')
plt.xlabel('Health Status')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

# Define features and target variable
features = ['Soil_Moisture', 'Ambient_Temperature', 'Soil_Temperature',
            'Humidity', 'Light_Intensity', 'Soil_pH',
            'Nitrogen_Level', 'Phosphorus_Level', 'Potassium_Level',
            'Chlorophyll_Content', 'Electrochemical_Signal']

X = df[features]
y = df['Plant_Health_Status']
print(f"Features shape: {X.shape}")
print(f"Target shape: {y.shape}")

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"\nTraining set size: {X_train.shape[0]}")
print(f"Testing set size: {X_test.shape[0]}")

# Check the distribution in training and test sets
print("\nClass distribution in training set:")
print(y_train.value_counts(normalize=True))
print("\nClass distribution in test set:")
print(y_test.value_counts(normalize=True))

# Initialize and train the Random Forest model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
print("Model training completed!")

# Make predictions
y_pred = model.predict(X_test)
print("Predictions made on test data!")

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"\nModel Accuracy: {accuracy:.2f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Feature importance
feature_importance = pd.DataFrame({
    'feature': features,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

print("\nFeature Importance (most to least important):")
print(feature_importance)

# Plot feature importance
plt.figure(figsize=(10, 6))
sns.barplot(x='importance', y='feature', data=feature_importance)
plt.title('Feature Importance in Plant Health Prediction')
plt.tight_layout()
plt.show()

# Confusion matrix
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=model.classes_, yticklabels=model.classes_)
plt.title('Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

# Re-split the data with 30% test size
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(f"New training set size: {X_train.shape[0]}")
print(f"New testing set size: {X_test.shape[0]}")

# Re-initialize and train the Random Forest model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
print("Model re-trained with 30% test split!")

# Make new predictions
y_pred = model.predict(X_test)

# Evaluate the model with the new split
accuracy = accuracy_score(y_test, y_pred)
print(f"\nModel Accuracy with 30% test split: {accuracy:.4f} ({(accuracy*100):.2f}%)")

print("\nDetailed Classification Report:")
print(classification_report(y_test, y_pred))


# Check feature importance again
feature_importance = pd.DataFrame({
    'feature': features,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

print("\nFeature Importance with 30% test split:")
print(feature_importance)


# Confusion matrix for the new split
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=model.classes_, yticklabels=model.classes_)
plt.title('Confusion Matrix (30% Test Split)')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

from sklearn.metrics import accuracy_score

# Assuming you have true labels and predicted labels
y_true = [0, 1, 0, 1, 0, 1]  # True labels
y_pred = [0, 1, 0, 0, 0, 1]  # Predicted labels

# Calculate accuracy
accuracy = accuracy_score(y_true, y_pred)
print(f"Accuracy: {accuracy:.2f}")

from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
# Set visual style
plt.style.use('default')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (10, 6)
# ======================
# LOAD AND EXPLORE DATA
# ======================
print("ðŸŒ± MYCO-NET: AI FUNGAL NETWORK INTERPRETER")
print("=" * 50)

# Load the Tree Survival Prediction 2 dataset
df = pd.read_csv("Tree_Data (1).csv")



# Explore the dataset
print("Dataset Shape:", df.shape)
print("\nFirst 5 rows:")
print(df.head())
print("\nColumn names:")
print(df.columns.tolist())
print("\nBasic info:")
print(df.info())

# Check the key columns for your project
print("Microbe treatments distribution:")
print(df['Microbe'].value_counts())

print("\nAMF colonization values:")
print(df['AMF'].describe())

print("\nSpecies distribution:")
print(df['Species'].value_counts())

print("\nLight levels:")
print(df['Light'].value_counts())

print("\nSurvival events (Target variable):")
print(df['Event'].value_counts())


# ======================
# DATA PREPROCESSING
# ======================
print("\n" + "=" * 50)
print("DATA PREPROCESSING")
print("=" * 50)

# Check missing values
print("Missing Values in Key Columns:")
missing_data = df[['Species', 'Light', 'Microbe', 'AMF', 'Event']].isnull().sum()
print(missing_data)

# Filter for complete cases of our key features
myco_features = ['Species', 'Light', 'Microbe', 'AMF', 'PHN_Imp', 'NSC_Imp', 'LIG_Imp']
target = 'Event'

myco_df = df[myco_features + [target]].dropna()
print(f"\nData shape after focusing on fungal features: {myco_df.shape}")

# Drop rows with missing values in these key columns
myco_df = df[myco_features + [target]].dropna()

print(f"Data shape after focusing on fungal features: {myco_df.shape}")

# ======================
# FEATURE ENGINEERING
# ======================
print("\n" + "=" * 50)
print("FEATURE ENGINEERING")
print("=" * 50)

# Encode categorical features
le_species = LabelEncoder()
le_light = LabelEncoder()
le_microbe = LabelEncoder()

myco_df['Species_encoded'] = le_species.fit_transform(myco_df['Species'])
myco_df['Light_encoded'] = le_light.fit_transform(myco_df['Light'])
myco_df['Microbe_encoded'] = le_microbe.fit_transform(myco_df['Microbe'])

# Define final features and target
final_features = ['Species_encoded', 'Light_encoded', 'Microbe_encoded', 'AMF', 'PHN_Imp', 'NSC_Imp', 'LIG_Imp']
X = myco_df[final_features]
y = myco_df[target]

print("Final feature set shape:", X.shape)
print("Target shape:", y.shape)

# ======================
# DATA VISUALIZATION
# ======================
print("\n" + "=" * 50)
print("DATA VISUALIZATION")
print("=" * 50)

# Create subplots
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# Plot 1: AMF Distribution
axes[0, 0].hist(myco_df['AMF'], bins=30, color='lightgreen', edgecolor='black')
axes[0, 0].set_title('Distribution of Fungal Colonization (AMF)')
axes[0, 0].set_xlabel('AMF Colonization (%)')
axes[0, 0].set_ylabel('Frequency')

# Plot 2: Survival by Microbe Treatment
microbe_survival = pd.crosstab(myco_df['Microbe'], myco_df['Event'])
microbe_survival.plot(kind='bar', ax=axes[0, 1])
axes[0, 1].set_title('Survival by Microbial Treatment')
axes[0, 1].set_xlabel('Microbial Treatment')
axes[0, 1].set_ylabel('Count')
axes[0, 1].legend(['Survived', 'Died'])

# Plot 3: Survival by Light Condition
light_survival = pd.crosstab(myco_df['Light'], myco_df['Event'])
light_survival.plot(kind='bar', ax=axes[1, 0])
axes[1, 0].set_title('Survival by Light Condition')
axes[1, 0].set_xlabel('Light Condition')
axes[1, 0].set_ylabel('Count')
axes[1, 0].legend(['Survived', 'Died'])

# Plot 4: Survival by Species
species_survival = pd.crosstab(myco_df['Species'], myco_df['Event'])
species_survival.plot(kind='bar', ax=axes[1, 1])
axes[1, 1].set_title('Survival by Plant Species')
axes[1, 1].set_xlabel('Plant Species')
axes[1, 1].set_ylabel('Count')
axes[1, 1].legend(['Survived', 'Died'])

plt.tight_layout()
plt.show()

# ======================
# MODEL TRAINING
# ======================
print("\n" + "=" * 50)
print("MODEL TRAINING")
print("=" * 50)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

print(f"Training set: {X_train.shape[0]} samples")
print(f"Testing set: {X_test.shape[0]} samples")

# Train Random Forest model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate model
accuracy = accuracy_score(y_test, y_pred)
print(f"Myco-Net Model Accuracy: {accuracy:.4f} ({(accuracy*100):.2f}%)")

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Feature importance for Myco-Net
feature_importance = pd.DataFrame({
    'feature': final_features,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

print("\nMyco-Net Feature Importance:")
print(feature_importance)

# Visualize feature importance
plt.figure(figsize=(10, 6))
sns.barplot(x='importance', y='feature', data=feature_importance)
plt.title('Myco-Net: Feature Importance in Predicting Plant Survival')
plt.tight_layout()
plt.show()

# Confusion matrix with all required imports

plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix: Myco-Net Predictions')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()


# ======================
# INTERPRETATION & INSIGHTS
# ======================
print("\n" + "=" * 50)
print("MYCO-NET INSIGHTS")
print("=" * 50)

print("The AI learned to predict plant survival based on:")
for i, row in feature_importance.iterrows():
    print(f"{i+1}. {row['feature']} (Importance: {row['importance']:.3f})")

print("\nðŸ”¬ Microbial Treatments Decoded:")
for i, category in enumerate(le_microbe.classes_):
    treatment_type = ""
    if category == "Large":
        treatment_type = " - Contains beneficial mycorrhizal fungi"
    elif category == "Small":
        treatment_type = " - Contains potentially pathogenic microbes"
    elif category == "Combined":
        treatment_type = " - Mixed microbial community"
    elif category == "Control":
        treatment_type = " - Sterilized control"
    print(f"  {i}: {category}{treatment_type}")

print(f"\nðŸ’¡ Key Insight: Microbial communities and fungal networks account for "
      f"{(feature_importance.iloc[0]['importance'] + feature_importance.iloc[1]['importance']):.1%} "
      "of the predictive power!")

# ======================
# PRACTICAL APPLICATION
# ======================
print("\n" + "=" * 50)
print("PRACTICAL APPLICATION FOR FARMERS")
print("=" * 50)

print("Based on our findings, Myco-Net can help farmers by:")
print("1. Monitoring soil microbiome health through simple sensors")
print("2. Alerting when pathogenic microbes dominate the soil")
print("3. Recommending interventions to support beneficial fungi")
print("4. Predicting crop survival probability based on fungal network strength")
print("5. Optimizing light conditions for different plant species")

print("\nðŸšœ Example farmer alert:")
print("   'Alert: Pathogenic microbes detected in Sector B. ")
print("    Fungal network strength is low (AMF: 15%). ")
print("    Recommended action: Apply mycorrhizal inoculant.'")

from sklearn.metrics import accuracy_score

# Assuming you have true labels and predicted labels
y_true = [0, 1, 0, 1, 0, 1]  # True labels
y_pred = [0, 1, 0, 0, 0, 1]  # Predicted labels

# Calculate accuracy
accuracy = accuracy_score(y_true, y_pred)
print(f"Accuracy: {accuracy:.2f}")
